%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% University Assignment Title Page 
% LaTeX Template
% Version 1.0 (27/12/12)
%
% This template has been downloaded from:
% http://www.LaTeXTemplates.com
%
% Original author:
% WikiBooks (http://en.wikibooks.org/wiki/LaTeX/Title_Creation)
%
% License:
% CC BY-NC-SA 3.0 (http://creativecommons.org/licenses/by-nc-sa/3.0/)
% 
% Modified for COSC480/490 by:
% Lech Szymanski (8/3/18)

\documentclass[12pt]{article}
\usepackage[draft]{cosc4x0style}
%\usepackage{cosc4x0style}
\usepackage[T1]{fontenc}

\usepackage{hyperref}
\usepackage{fvextra}
\usepackage{csquotes} 
\usepackage{biblatex}
\usepackage{listings}
\usepackage{float}
\usepackage{array}
\usepackage{threeparttable}
\usepackage{multirow}
\usepackage{booktabs}
\usepackage{dirtytalk}
\usepackage{xcolor}
\usepackage{soul}
\addbibresource{refs.bib}
\usepackage{listings}
\usepackage[margin=1in]{geometry}
% To compile the final version of the report (which will remove all the todo content)

% Specify project code 480 or 490 \papercode{480}

% Your project title
\title{An Exploration of the Common Vulnerability Scoring System}

% Your name
\author{Jake \textsc{Norton}}
\studentid{5695756}

% Names of your supervisors, separated by line break '\\'
\supervisors{
  Dr.\@ David \textsc{Eyers} \\
  Dr.\@ Veronica \textsc{Liesaputra}
}

% Date, change the \today to a set date if you want to be precise
\reportdate{\today}

% Dave's editing macros
\newcommand{\note}[2][red]{\textcolor{#1}{#2}}
\newcommand{\notedme}[1]{\note[blue]{[<Dave> #1]}}
\newenvironment{scaffold}{\color{red}}{}
\newcommand{\change}[2][]{\textcolor{orange}{#2}}

\begin{document}


\maketitle

\begin{abstract}

	The Common Vulnerability Scoring System (CVSS) is designed to produce scores for software
	vulnerabilities, serving as a crucial tool for triaging the increasing number of new
	vulnerabilities released each year. As manual scoring cannot keep pace with this growth, there
	is a need for automated prediction methods. While machine learning, particularly large language
	models (LLMs), have shown promise in predicting CVSS metrics given there descriptions. This
	paper aims to explore the CVE and CVSS processes to provide a foundation for future research.
	This paper explores the potential of using multiple data sources for cross-validation and
	increased training data volume, specifically comparing NVD with the MITRE database. The analysis
	reveals differences in how these databases rate Common Vulnerabilities and Exposures (CVEs),
	highlighting the challenges in establishing a consistent ground truth for CVSS scores. While the
	combination of multiple datasets did not yield the expected improvements in prediction accuracy,
	this investigation provides valuable insights into the complexities of CVSS scoring across
	different evaluation teams. The next focus of this paper is on enhancing the interpretability
	and analysis of CVE/CVSS data through clustering techniques and other dataset analysis methods.
	I employ Latent Dirichlet Allocation (LDA) and other clustering approaches to uncover patterns
	within the vulnerability descriptions. The findings demonstrate that certain types of vulnerabilities
	tend to cluster together, such as cross-site scripting, denial-of-service attacks.
\end{abstract}


\section{Introduction}

In 2023, 29,065 new vulnerabilities were recorded, as illustrated in Figure~\ref{fig:cves_per_year}.
This number continues to rise year on year, underscoring the growing challenge of managing and
prioritizing software vulnerabilities. These vulnerabilities are documented using the Common
Vulnerabilities and Exposure system (CVE~\cite{CVE}), with CVSS scores derived from these entries to
indicate severity and impact. The National Vulnerability Database (NVD~\cite{NVD} serves as the
primary source for CVSS-enriched CVE data and is widely used in research (e.g., \cite{costa,
	nvd_example1, nvd_example2}). However, to explore the potential benefits of multiple data sources,
we also investigated the MITRE database~\cite{MITRE}, which is the main repository for CVEs and
includes a significant number of entries with CVSS scores.The initial comparison between NVD and
MITRE databases employed a methodology inspired by the paper \say{Can the Common Vulnerability
	Scoring System be Trusted? A Bayesian Analysis}~\cite{bayes}. This approach aimed to assess the
agreement between different data sources in scoring CVEs and explore the feasibility of establishing
a ground truth. The analysis revealed differences in how these databases rate CVEs (see Figure
\ref{fig:counts}, \ref{fig:mitre_31_confusion_matrices}, \ref{fig:nvd_31_confusion_matrices}), highlighting the subjective nature of CVSS scoring and the challenges in
achieving consistency across different evaluation teams. While the comparison of multiple datasets
did not yield the anticipated improvements in CVSS prediction, it provided valuable insights into
the variability of human-assigned scores. This variability suggests that automated prediction models
may face similar challenges in areas where human evaluators disagree. Given these findings, the
focus of the research shifted towards enhancing the interpretability and analysis of the CVE/CVSS
data itself. I employed clustering techniques, primarily Latent Dirichlet Allocation
(LDA~\cite{lda_origin}), along with other dataset analysis methods to uncover patterns within the
vulnerability descriptions. This approach offers several advantages:

\begin{enumerate}

	\item It provides a data-driven perspective that complements the LLM-based prediction methods.

	\item The clustering techniques offer faster analysis times compared to complex LLM interpretability
	      methods, with LDA processing the entire corpus in approximately one hour.

	\item It reveals inherent groupings and relationships within the data that may not be immediately apparent
	      through other analysis methods.

\end{enumerate}

The clustering analysis has yielded interesting insights, such as the tendency for certain types of
vulnerabilities to cluster together. For example the analysis of the integrity impact metric revealed meaningful patterns that align with expert knowledge in the field:

\todo[inline]{Unsure if this makes sense here as I haven't introduced what all the CVSS terms mean
	yet.... also repetition for later..}
\begin{itemize}

	\item Vulnerabilities classified as having no impact (\texttt{NONE}) on data integrity often
	      clustered around denial of service attacks and system crashes. While these are serious
	      issues, they typically do not directly affect data integrity.

	\item Low impact (\texttt{LOW}) on integrity was frequently associated with cross-site scripting
	      vulnerabilities, particularly in WordPress plugins. This classification makes sense as such
	      vulnerabilities can cause localized data integrity issues, often limited to individual user
	      interactions rather than compromising the entire application.

	\item High impact (\texttt{HIGH}) integrity issues clustered around SQL injection and
	      database-related vulnerabilities. This aligns with the severe nature of these attacks, which
	      can directly manipulate or corrupt data at the database level.

\end{itemize}

The following report will try to answer these questions:

\begin{itemize}
	\item Is there scoring consistency between MITRE and NVD?

	\item Can we improve the LLMs classifier performance when we augment the NVD dataset with data
	      from the MITRE dataset?

	\item What are the important keywords for each category?
\end{itemize}

In the following sections, I will detail my methodology, present my findings from both the
database comparison and clustering analyses, and discuss the implications of these results for the
field of vulnerability assessment and management.


\begin{figure}[ht] \centering
	\includegraphics[width=0.8\textwidth]{figures/cves_per_year.pdf}
	\caption{\label{fig:cves_per_year}Number of new CVEs by year}
\end{figure}


\section{Background}

Vulnerabilities are stored in a consistent system called Common Vulnerabilities and
Exposures (CVE~\cite{CVE}).

\subsubsection*{Here is an example CVE}
\begin{itemize}

	\item   Unique Identifier: CVE-2024-38526

	\item   Source: GitHub, Inc.

	\item   Published: 06/25/2024

	\item   Updated: 06/26/2024

	\item   Description: pdoc provides API Documentation for Python Projects. Documentation
	      generated with \texttt{pdoc --math} linked to JavaScript files from polyfill.io. The polyfill.io
	      CDN has been sold and now serves malicious code. This issue has been fixed in pdoc 14.5.1.

\end{itemize}

{\footnotesize Sourced from \href{https://nvd.nist.gov/vuln/detail/CVE-2024-38526}{NVD CVE-2024-38526
	Detail} \cite{polyfill}} \\
\bigskip

This has a unique identifier, which is given by one of the CVE numbering authorities (CNA~\cite{CNA}), such as
GitHub, Google or any of these, \href{https://www.cve.org/PartnerInformation/ListofPartners}{CVE list of
	partners}~\cite{partners}.
The description is the most important part in our case. This should provide information about the
vulnerability. What can be exploited (device / software component)? How is the product affected if
the vulnerability is exploited? Ideally there would be a part of the description that relates to every metric,
unfortunately these descriptions are not necessarily suited to machine learning as the people
writing the descriptions are expecting a lot of intrinsic knowledge.

\subsection*{The Common Vulnerability Scoring System}

CVSS scoring is a high level way to break up vulnerabilities into different categories.
Organisations can use it to choose which vulnerability to focus on first. CVSS is broken up into
three distinct sections: base, temporal and environmental scores.

For brevity I will only show the specifics of CVSS 3.1~\cite{CVSS_31} as this is by far the most commonly used
version, even if it is not the most recent.

\subsubsection*{Base Score}

\begin{itemize}

	\item Attack Vector: Defines the avenues of attack that the vulnerability is open to. The more
	      open a component is, the higher the score. This can have the values: \texttt{Network},
	      \texttt{Adjacent}, \texttt{Local} and \texttt{Physical}.

	\item Attack Complexity: Describes how complex the attack is to orchestrate. Encompasses
	      questions like, what are the prerequisites? How much domain knowledge / background work is
	      necessary? How much effort does the attacker need to invest to succeed? This can have the
	      values: \texttt{Low} or \texttt{High}. \texttt{Low} gives a higher base score.

	\item Priviledges Required: The degree of privileges the user needs to complete the attack.
	      Ranging from: \texttt{None}, \texttt{Low} (e.g.\@ User level privilege), \texttt{High}
	      (e.g.\@ Administrator). The lower the privilege the higher the base score.

	\item User Interaction: Describes if the exploit requires another human user to make the attack
	      possible, E.g.\@ clicking a phishing link. This is either \texttt{None} or
	      \texttt{Required}, the score is highest when no user interaction is required.

	\item Scope: Defines if the attack can leak into other security scopes. E.g.\@~access to one
	      machine gives the ability to elevate privileges on other parts of the system. This can take
	      \texttt{Unchanged} or \texttt{Changed}, the score being highest when a scope change occurs.

	\item Confidentiality Impact: Detemines what is the impact on the information access /
	      disclosure to the attacker. This can be: \texttt{High}, \texttt{Low} or \texttt{None} with
	      \texttt{High} adding the most to the base score.

	\item Integrity Impact: Refers to the integrity of the information within the component. I.e.\@
	      could the data have been modified by the attacker. This has: \texttt{High}, \texttt{Low} or
	      \texttt{None}, as categories with \texttt{High} adding the most to the base score.

	\item Availability Impact: Refers to the impact of the attack on the availability of the
	      component. E.g.\@ the attacker taking the component off the network, denying the users
	      access. This can be: \texttt{High}, \texttt{Low} and \texttt{None} with \texttt{High} adding
	      the most to the base score.

\end{itemize}

{\footnotesize This is a summarized version of the
\href{https://www.first.org/cvss/v3.1/specification-document}{3.1 specification document
	provided by the  Forum of Incident Response and Security Teams (FIRST)}~\cite{CVSS_31}. } \\

\subsubsection*{Temporal}

\begin{itemize}

	\item Exploit Code Maturity: The state of the attack itself, e.g.\@ has this exploit been pulled
	      off in the wild or is it currently academic.

	\item Remediation Level: Whether the exploit in question has a patch available.

	\item Report Confidence: The degree of confidence in the CVE report itself, the report may be in
	      early stages where not all of the information is known.

\end{itemize}

{\footnotesize This is a summarized version of the
\href{https://www.first.org/cvss/v3.1/specification-document}{3.1 specification document
	provided by the Forum of Incident Response and Security Teams (FIRST)}~\cite{CVSS_31}.}  \\
\bigskip

Temporal metrics would be useful in general for a CVSS score, however NVD do not store these
temporal metrics. As far as I can tell there is no reason given for this specifically, though
discourse
\href{https://security.stackexchange.com/questions/270257/cvss-v3-and-v3-1-missing-temporal-metrics-exploit-code-maturity-and-remediation}{(Stack exchange post)}~\cite{stack_exchange} around the subject suggests that this is due to a lack
of verifiable reporting. From my perspective, both remidiation level and report confidence feel like
they could have scores attributed to them, however finding verifiable reports on the exploits seen
in the wild is difficult. There are two relatively new organisations on this front,
Cybersecurity \& Infrastructure Security Agency (CISA,
\href{https://www.cisa.gov/known-exploited-vulnerabilities-catalog}{public sector}) and
inthewild.org (\href{https://inthewild.io/}{private sector}~\cite{cisa}).

\subsection{Data Options}

I will be using NVD and MITRE as the sources of data. In 2016 when Johnson et al.\@ did their paper
on CVSS~\cite{bayes}, they had access to five different databases. Unfortunately only two of these
remain for modern data. There are others, but they are either in archival or proprietary status.

\subsubsection{National Vulnerability Database} \label{NVD_SECTION}

The National Vulnerability Database is the defacto standard dataset used for CVSS generation
research~\cite{costa, nvd_example1, nvd_example2}.  This makes a lot of sense as it is built
for the purpose with a team dedicated to enriching CVEs with CVSS scores. The dataset I am using was
retrieved using the NVD API in March 2024 and contains $\sim$100000 CVEs enriched with CVSS scores. This
comes in a consistently formatted JSON dump.

\subsubsection{MITRE Database}  \label{MITRE_SECTION}

MITRE is the defacto database for the storage of CVEs themselves, their database contains $\sim$40000
CVEs enriched with CVSS 3.1 scores. These are in a JSON dump retrieved in March 2024. The
format for usage is a bit more cumbersome to use. The CVSS scores are only stored as CVSS vector
strings (a simple text encoding~\cite{vector_string}). These are not hard to parse, though they are stored slightly
different between versions, as well as sometimes being inconsistent ($\sim$5000 had temporal metrics within
the vector strings in the MITRE database).



\begin{figure}
	\centering
	\makebox[\textwidth][c]{\includegraphics[width=1.2\textwidth]{figures/combined_overlap.pdf}}
	\caption{\label{fig:counts}Comparison of CVSS ratings between MITRE and NVD}
\end{figure}

\begin{figure}
	\centering
	\makebox[\textwidth][c]{\includegraphics[width=0.8\textwidth]{figures/nvd_data.pdf}}
	\caption{\label{fig:nvd_data}Distribution of Metric ratings for NVD}
\end{figure}


\subsection{Evolution of CVSS and Its Identity Crisis}
\todo[inline]{Possibly need some kind of link between these...}
When CVSS 2.0 was released, it was promoted as a framework to help IT management prioritize and
remediate vulnerabilities posing the greatest risk. The initial goal was to provide a comprehensive
method for assessing risk, as indicated by its original documentation:
\\

\textit{\say{Currently, IT management must identify and assess vulnerabilities across many disparate
		hardware and software platforms. They need to prioritize these vulnerabilities and remediate those
		that pose the greatest risk. But when there are so many to fix, with each being scored using
		different scales, how can IT managers convert this mountain of vulnerability data into
		actionable information? The Common Vulnerability Scoring System (CVSS) is an open framework that
		addresses this issue.}}~\cite{CVSS_2}
\\

However, by the time CVSS 3.1 was released, the framework's focus had shifted, partly due to
complaints about CVSS being a poor judge of risk. The authors stated:

\textit{\say{CVSS measures severity, not risk.}}~\cite{CVSS_31}
\\

The identity crisis is a problem because the original stance, that it can be used as a primary
prioritisation tool, has lured parts of the industry into doing just that. As mentioned by Henry
Howland in~\cite{ubiquitous}, there are many large, mainly US based places mandating the sole use of
CVSS base score for remediation, such as the Payment Card Industry Data Security Standard~\cite{PCI}, the
Department of Defense Joint Special Access Program implementation Guide~\cite{DOD} to name a few.

This change in stance has created confusion about the true purpose of CVSS.

\subsubsection{CVSS Formula}


How CVSS is computed under-the-hood is confusing at best. CVSS 3.1 is not explained to the same
depth as version 2.0, but my understanding is that it followed a similar process. This is that
process summarised from
\href{https://www.first.org/cvss/v2/faq#Explanation-of-CVSS-v2-formula-and-metric-valued-development}{CVSS
	version 2 FAQ:}~\cite{CVSS_formula}


\begin{enumerate}
	\item  Divide the six metrics into two groups:

	      \begin{itemize}
		      \item \textbf{Impact} (3 metrics)
		      \item \textbf{Exploitability} (3 metrics)
	      \end{itemize}

	\item Create sub-vectors for each group:
	      \begin{itemize}
		      \item \textbf{Impact sub-vector}: 27 possible values ($3^3$)
		      \item \textbf{Exploitability sub-vector}: 26 possible values ($3^3 - 1$ for no impact)
	      \end{itemize}

	\item Develop and apply a series of rules to order the sub-vectors. These rules
	      are primarily based on the severity of components, e.g.\@ vectors with more
	      \texttt{Complete}
	      components are rated higher.

	\item Assign scores to the ordered sub-vectors based on the derived rules and review by the
	      Special Interest Group (SIG).

	\item Apply weightings to the sub-vectors:
	      \begin{itemize}
		      \item \textbf{Impact}: 0.6
		      \item \textbf{Exploitability}: 0.4
	      \end{itemize}

	\item \label{formula} Develop a formula that approximates the scores derived from the ordered
	      sub-vectors. Ensure the formula produces scores with $\pm$0.5 error from the originally
	      defined vector score and does not exceed the maximum value of 10.

	\item Test and refine the formula through iterations, ensuring it aligns with desired values and
	      corrects any issues, such as scores exceeding 10.

\end{enumerate}

This process is inherently inaccurate, it is not a system designed to give precise scores. If we
look at~\ref{formula} above, the formula (\ref{equation} shows the CVSS 3.1 base score formula for
reference) which produces the score, does not match exactly the experts decision. There is a lot of
rounding and approximation going on. This is designed to make a system which is easy to use and
quick to complete by security professionals. There is a space for CVSS, however this along with the
other mentioned reasons outlines the issues with using CVSS as a sole metric for prioritization. Perhaps an option
is to triage the large swathes of new vulnerabilites coming in with an initial CVSS score, then move
on to a deeper dive. This could be in the form of the extra CVSS metrics (Temporal score \&
Environmental score), or a look into other potential options like the Exploit Prediction Scoring
System (EPSS~\cite{EPSS}).

\subsection{Related Work}

\todo[inline]{Need to add the new papers relevant to the new sections to here....}
The main paper most similar to the Bayesian Analysis between MITRE and NVD is
\textit{Can the Common Vulnerability Scoring System be Trusted? A Bayesian
	Analysis}~\cite{bayes} by Johnson et al.\@. They conducted a study into the state of CVSS databases and their accuracy in 2016.
They found NVD to be the most correct database, and we can trust the Common~Vulnerability~Scoring~System as a
whole as scorers rate CVEs consistently. This paper will be used as the basis for the database
comparison in Section~\ref{bayesian_modeling}. This continues to be relevant in terms of process,
not so much in terms of results.

Costa et al.\@ in the paper, \textit{Predicting CVSS Metric via Description
	Interpretation}~\cite{costa}, tested generation of CVSS from CVE descriptions with a range of large
language encoder-only models. They achieved state-of-the-art results with the DistilBERT
model~\cite{distilbert}. They also improved the score with text preprocessing (e.g.\@ lemmatization)
and looked into interpretability of the models using Shapley values. This paper is relevant to
Section~\ref{cvss_prediction} as much of the process around training and inferring CVSS scores is
based on their work.

Jiang and Atif in the paper, \textit{An Approach to Discover and Assess
	Vulnerability Severity Automatically in Cyber-Physical Systems}~\cite{jiang}, create a novel
pipeline for vulnerability assessment. They used multiple data sources and a majority voting system
to decrease the chance of badly scored CVEs. This paper relates in that it shows a use case for
multiple data sources, though less so on the methods used.

% Henry Howland in the paper, \textit{CVSS: Ubiquitous and Broken}~\cite{ubiquitous} broke down issues with
% the CVSS system, namely, \say{lack of justification for its underlying formula, inconsistencies
% 	in its specification document, and no correlation to exploited vulnerabilities in the wild,
% 	it is unable to provide a meaningful metric for describing a vulnerability’s
% 	severity}~\cite{ubiquitous}.  This paper mainly relates to Section~\ref{discussion} exposing the
% issues and general impression of CVSS.

\section{Exploratory Study on NVD and MITRE}



\subsubsection{Priliminary Data exploration}

The scorers for both NVD and MITRE do rate CVEs reasonably similar, one pattern you can see as shown
by Figure~\ref{fig:counts}, is that NVD generally give the most common categorical output more ratings.
They are less spread out across the full range of values. In addition, if we look at the
\texttt{Attack Complexity} metric, there is a reasonably large difference in how they are rated,
MITRE rate a lot more of the metrics with a \texttt{Low} score. This points to some of the
difficulty with this kind of rating system, while in theory there is a true value for these metrics,
it requires knowledge of the whole space around each of the vulnerabilities, this knowledge will
always vary marker to marker. The model will not have direct access to this knowledge; however, it
is hoped that it will be able to trace relationships between the different vulnerabilities and learn
this intrinsically.

\subsection{Hierarchical Bayesian Model} \label{bayesian_modeling}

Analysis between the NVD~\cite{NVD} and MITRE~\cite{MITRE} databases is conducted using a
hierarchical Bayesian model. This model type is particularly suitable when the population is
expected to share similarities in some respects while differing in others. In this context, while
the databases share common knowledge about vulnerabilities, they differ in the experience of the
individuals rating the metrics~\cite{bayes}.

The model builds upon the original framework presented in Section 4.1 of~\cite{bayes}. It assumes
the existence of a true value for each CVSS dimension, acknowledging that the database sample may
deviate from this true value. These potential inaccuracies are represented using confusion matrices
(see Equation~\ref{confusion_matrix_formula}).

A key distinction from the original model is the variability in the number of categorical choices
for each CVSS metric. While the original model consistently used three variables for each CVSS
metric, my adapted model accommodates between two to four categorical choices, depending on the
specific CVSS dimension being evaluated.

\subsubsection{Confusion Matrix}

Below is an example of the confusion matrix for the CVSS dimension \texttt{Confidentiality Impact}:

\begin{equation}\label{confusion_matrix_formula}
	\Pi_{ci} = \begin{bmatrix}
		\pi_{nn} & \pi_{nl} & \pi_{nh} \\
		\pi_{ln} & \pi_{ll} & \pi_{lh} \\
		\pi_{hn} & \pi_{hl} & \pi_{hh} \\
	\end{bmatrix}
\end{equation}

where $\pi_{nn}$ denotes the probability that the database correctly assigns the score \texttt{None}
when the actual value is indeed \texttt{None}. Conversely, $\pi_{nl}$ and $\pi_{nh}$ represent
instances where \texttt{None} was not the true value, indicating an incorrect score assignment by
the database.

\subsubsection{Prior Distributions}

For categorical variables, we employ uninformative priors using Dirichlet distributions\cite{dirichlet}. This
approach provides a uniform prior over the probability space for all categorical options, minimizing
the influence of prior beliefs on the results. The impact of these priors is negligible for
categorical metrics with more options than the number of categories.

The confusion matrices also utilize Dirichlet distribution priors. However, we incorporate a slight
initial belief to reflect that the individuals producing scores are not acting entirely randomly and
are likely to be correct more often than not. These priors remain weak given the thousands of
observations in our dataset.

\subsubsection{Parameter Estimation}

Our parameter estimation follows the Bayesian approach, updating prior beliefs with observations to
produce posterior beliefs. We employ Markov Chain Monte Carlo (MCMC~\cite{mcmc}) methods, which allow for
simulating data based on the previously created distribution by sampling values that the model
believes are likely from the target distribution. This technique enables accurate sampling without
requiring all data points.

For implementation, we utilized the PyMC library~\cite{pymc}, which fulfills the same functions as
JAGS in the original paper~\cite{bayes}. PyMC facilitates the modeling process and provides robust
tools for Bayesian inference and MCMC sampling.

This methodology provides a comprehensive framework for analyzing and comparing vulnerability
scoring across different databases, accounting for the inherent uncertainties and variations in
expert assessments.


\subsubsection{Bayesian Analysis Results}

The results for the database analysis will be shown through the confusion matrices for the estimated
accuracy of both NVD and MITRE. Unfortunately it is difficult to compare my results to the original
paper~\cite{bayes} as they are on a totally different dataset. However, I will note that in general the estimated
accuracy for both datasets is much lower than the scores from the original study. Across the board NVD
often had $\sim$90\% accuracy for the highest accuracy field of any metric, with Confidentiality as
a clear outlier as seen from Table~\ref{fig:johnson_confusion_matrices}.

Some general trends are that NVD (see Figure~\ref{fig:nvd_31_confusion_matrices}) have more extreme
estimated accuracies. They do better for the higher frequency options, for the \texttt{Low} option
on \texttt{Attack Complexity} for example, NVD have 98\% estimated accuracy and 66\% for
\texttt{High} versus MITRE (see Figure~\ref{fig:mitre_31_confusion_matrices}), \texttt{Low} scored
87\% and 75\% for \texttt{High}. Instead of further analysing in this way, I will point out some of
my worries around these results, not that they are wrong per se, but that they do not really tell us
anything extra than that shown by in Figure~\ref{fig:counts}, except perhaps it is helpful to see
the results in a percentage estimated accuracy instead of a proportion. Unfortunately this is
outside of my strengths, I did some cursory exploration into if doing this sort of analysis
between two populations like this does make sense, the discourse here~\cite{stat_modeling} suggests
that such a thing can be done, though it also suggests the need for more informative priors. This
may apply in my case, and I intend on getting an expert opinion closer to home, however that will be
after this report.

\bigskip

\begin{figure}[H]
	\centering
	\makebox[\textwidth][c]{\includegraphics[width=1.3\textwidth]{figures/nvd_31_titles.pdf}}
	\caption{\label{fig:nvd_31_confusion_matrices}Confusion Matrices for NVD for CVSS version 3.1}
\end{figure}

\begin{figure}[H]
	\centering
	\makebox[\textwidth][c]{\includegraphics[width=1.3\textwidth]{figures/mitre_31_titles.pdf}}
	\caption{\label{fig:mitre_31_confusion_matrices}Confusion Matrices for MITRE for CVSS version 3.1}
\end{figure}

\begin{table}
	\label{fig:johnson_confusion_matrices}
	\centering
	\caption{Confusion Matrices for NVD on CVSS version 2.0 from Johnson et al.\@ \cite{bayes}}
	\[
		\begin{array}{c|c}
			\textbf{Access vector}  & \textbf{Access complexity} \\
			\hline
			\begin{tabular}{l|ccc}
				  & N    & A    & L    \\
				\hline
				N & 0.99 & 0    & 0    \\
				A & 0.21 & 0.71 & 0.08 \\
				L & 0.05 & 0.0  & 0.95 \\
			\end{tabular}
			                        &
			\begin{tabular}{l|ccc}
				  & L    & M    & H    \\
				\hline
				L & 0.54 & 0.29 & 0.17 \\
				M & 0.02 & 0.88 & 0.1  \\
				H & 0.01 & 0.08 & 0.91 \\
			\end{tabular}
			\\
			\textbf{Authentication} & \textbf{Confidentiality}   \\
			\hline
			\begin{tabular}{l|ccc}
				  & N    & S    & M    \\
				\hline
				N & 0.99 & 0.01 & 0    \\
				S & 0.04 & 0.95 & 0.01 \\
				M & 0.19 & 0.2  & 0.6  \\
			\end{tabular}
			                        &
			\begin{tabular}{l|ccc}
				  & C   & P    & N    \\
				\hline
				C & 0.4 & 0.2  & 0.2  \\
				P & 0.2 & 0.4  & 0.19 \\
				N & 0.2 & 0.21 & 0.4  \\
			\end{tabular}
			\\
			\textbf{Integrity}      & \textbf{Availability}      \\
			\hline
			\begin{tabular}{l|ccc}
				  & C    & P    & N    \\
				\hline
				C & 0.91 & 0.08 & 0.01 \\
				P & 0.05 & 0.93 & 0.02 \\
				N & 0.02 & 0.07 & 0.92 \\
			\end{tabular}
			                        &
			\begin{tabular}{l|ccc}
				  & C    & P    & N    \\
				\hline
				C & 0.92 & 0.07 & 0.01 \\
				P & 0.07 & 0.91 & 0.02 \\
				N & 0.02 & 0.11 & 0.87 \\
			\end{tabular}
		\end{array}
	\]
\end{table}


\section{CVSS Prediction}\label{cvss_prediction}
\todo[inline]{This section is still sort of a side part of the report, unsure if should be fleshed
	out a bit more or just left as is or something else as I haven't persured this aspect further}

Cody Airey --a classmate of mine-- has been working on a similar problem. He has been repoducing
results from Costa et al.\@~\cite{costa}, I am choosing to use Airey's results due to them being on
CVSS version 3.1 instead of a mix of versions 3.0 and 3.1 as Costa et al. used. My choice of model
for CVSS prediction will very much bootstrap off his work. So far, a strong contender for
state-of-the-art model for predicting CVSS metrics from CVE descriptions is the DistilBERT
model~\cite{distilbert}. This is a variant of BERT~\cite{BERT}, a pre-trained deep bidirectional
transformer large language model developed by Google. DistilBERT has advantages over other BERT
models in terms of performance, but also on speed of training as well as size / memory footprint of
the model.

\subsubsection{Training}

The model is trained separately for each metric. Each of the eight models were trained on five
different data splits to allow for a standard deviation to be calculated, in order to aid in
reducing the chance of a \textit{lucky} data split effecting the results. The difference between
Costa et al.\@ \& Airey's work, and mine is that my model was trained on a combination of NVD and
MITRE data as opposed to just using NVD. This was converted to a CSV containing the descriptions and
the CVSS scores. This does mean there are now $\sim$40000 duplicate CVEs and $\sim$140000 CVEs
enriched with CVSS scores total. Below, Table~\ref{tab:distil_part1} and Table~\ref{tab:distil_part2}
show the results of the DistilBERT model trained on a combination of NVD and MITRE data.
Unfortunately this has a purely negative effect on all metrics, with the caveat that some of the
standard deviations are lower. Additional note is that the balanced accuracy for some metrics looks
a bit weird, I believe that is due to the model not outputing some of the categorical options for
that metric. This applies to all the balanced accuracy scores, apart from the \texttt{Priorities
	Required} (PR) and \texttt{Confidentiality} (C) as seen on Table~\ref{tab:distil_part1} and
Table~\ref{tab:distil_part2}.
\todo[inline]{Finish rewriting the sections}
I hypothesize that this is due to the discrepancy between the two datasets on how a CVE is scored

% As to why the model performs worse, my theory is that the added data,
% and therefore the overlapping CVEs with different scores confuse the model. I thought that adding
% the additional data may have given the model a better chance of generalising, however this does not
% appear to be the case.

It suggests that the model may encounter similar challenges to those faced by human
evaluators. The lowest score for my model is in the \texttt{Availability Impact} category. This observation indicates a correlation between the machine learning models'
difficulties and the issues faced by human scorers. \texttt{Attack Complexity} also presents challenges,
likely due to data imbalance, as the dataset is heavily skewed towards the \texttt{Low} score,
thereby incentivizing the model to output \texttt{Low} scores (see
Figures~~\ref{fig:counts},~\ref{fig:nvd_31_confusion_matrices},~\ref{fig:mitre_31_confusion_matrices}
for reference).

\begin{table}[H]
	\centering
	\resizebox{\textwidth}{!}{%
		\begin{tabular}{llcccc}
			\toprule
			\textbf{Metric} & \textbf{Model}  & \textbf{AV}           & \textbf{AC}           & \textbf{PR}           & \textbf{UI}           \\
			\midrule
			\multirow{2}{*}{Accuracy}
			                & DistilBERT-Cody & \textbf{91.28 ± 0.26} & \textbf{95.64 ± 0.68} & \textbf{82.77 ± 0.24} & \textbf{93.86 ± 0.19} \\
			                & DistilBERT-Jake & 72.81 ± 0.32          & 92.62 ± 0.15          & 81.18 ± 0.18          & 66.35 ± 0.24          \\
			\midrule
			\multirow{2}{*}{F1}
			                & DistilBERT-Cody & \textbf{90.98 ± 0.31} & \textbf{93.85 ± 1.39} & \textbf{82.53 ± 0.26} & \textbf{93.82 ± 0.19} \\
			                & DistilBERT-Jake & 61.36 ± 0.42          & 89.08 ± 0.22          & 80.96 ± 0.19          & 52.93 ± 0.31          \\
			\midrule
			\multirow{2}{*}{Bal Acc}
			                & DistilBERT-Cody & \textbf{67.88 ± 2.11} & \textbf{55.82 ± 7.23} & \textbf{75.98 ± 0.47} & \textbf{92.46 ± 0.21} \\
			                & DistilBERT-Jake & 25.00 ± 0.00          & 50.00 ± 0.00          & 75.18 ± 0.31          & 50.00 ± 0.00          \\
			\bottomrule
		\end{tabular}
	}
	\caption{Comparison of the effects of the pre-trained models on the CVSS v3.1 dataset (Part 1).}
	\label{tab:distil_part1}
	\begin{tablenotes}
		\small
		\item Note: AV = Attack Vector, AC = Attack Complexity, PR = Privileges Required, UI = User Interaction
		\item Bal Acc = Balanced Accuracy
	\end{tablenotes}
\end{table}

\begin{table}[H]
	\centering
	\resizebox{\textwidth}{!}{%
		\begin{tabular}{llcccc}
			\toprule
			\textbf{Metric} & \textbf{Model}  & \textbf{S}            & \textbf{C}            & \textbf{I}            & \textbf{A}            \\
			\midrule
			\multirow{2}{*}{Accuracy}
			                & DistilBERT-Cody & \textbf{96.38 ± 0.09} & \textbf{86.24 ± 0.20} & \textbf{87.15 ± 0.10} & \textbf{88.70 ± 0.10} \\
			                & DistilBERT-Jake & 80.21 ± 0.16          & 82.45 ± 0.11          & 45.71 ± 0.26          & 52.53 ± 0.23          \\
			\midrule
			\multirow{2}{*}{F1}
			                & DistilBERT-Cody & \textbf{96.30 ± 0.10} & \textbf{86.09 ± 0.21} & \textbf{87.11 ± 0.10} & \textbf{88.04 ± 0.11} \\
			                & DistilBERT-Jake & 71.40 ± 0.22          & 82.34 ± 0.12          & 28.68 ± 0.28          & 36.18 ± 0.27          \\
			\midrule
			\multirow{2}{*}{Bal Acc}
			                & DistilBERT-Cody & \textbf{91.57 ± 0.43} & \textbf{82.70 ± 0.36} & \textbf{85.81 ± 0.10} & \textbf{64.01 ± 0.13} \\
			                & DistilBERT-Jake & 50.00 ± 0.00          & 79.85 ± 0.23          & 33.33 ± 0.00          & 33.33 ± 0.00          \\
			\bottomrule
		\end{tabular}
	}
	\caption{Comparison of the effects of the pre-trained models on the CVSS v3.1 dataset (Part 2).}
	\label{tab:distil_part2}
	\begin{tablenotes}
		\small
		\item Note: S = Scope, C = Confidentiality Impact, I = Integrity Impact, A = Availability
		Impact
		\item Bal Acc = Balanced Accuracy
	\end{tablenotes}
\end{table}

\section{Clustering}

\todo[inline]{Add in text to smooth over the transition between the two sections}
\todo[inline]{Maybe comparsion between my clusters and the CWEs those documents are clustered into?}

This next section is the continuation of the exploratory study with a focus on what are the
contributing factors for each metric / class for said metric. Clustering and analysis of the data
makes sense in many ways. As the data is already grouped into Common Weakness Enumeration~\cite{CWE}
, it is likely that there are some sort of patterns we can find. As a somewhat arbitrary place to
start, choose K as 8 due to that being the number of CVSS metrics. For this I was hoping to see the
data would naturally have some clusters based on the general theme of the vulnerability /
description, e.g. a cluster based generally around SQL injection / databases in general.


\subsection{K-Means Clustering of Vulnerability Descriptions}

The process of clustering vulnerability descriptions using K-means consists of four main steps:

\subsubsection*{Data Preparation}

The analysis begins by loading vulnerability descriptions from a dataset and using TF-IDF
(Term Frequency-Inverse Document Frequency) vectorization to convert the text
descriptions into numerical features~\cite{tfidf}. This technique helps capture the
importance of words within the context of the entire corpus often used in information
retrieval.


\subsubsection*{Modelling}

\begin{itemize}

	\item Standard K-Means algorithm is utilized for clustering. This method aims to partition the
	      descriptions into 8 clusters, each representing a group of similar
	      vulnerabilities.

	\item The K-Means algorithm operates by iteratively assigning data points to the nearest cluster
	      center and then updating the center based on the assigned points. The distance between
	      data points is evaluated using the Euclidean distance between the TF-IDF vectors.

\end{itemize}

\subsubsection*{Evaluation}


To assess the stability of the results, the clustering process is repeated multiple times
with different random seeds. I did record a coherency score, but as this was just for exploratory
purposes, the important part was if there was at least some indicator of this being a positive
direction of inquiry.


\subsubsection*{Interpretation}

Below are the example topics gained from the initial kmeans clustering:
\begin{itemize}

	\item Cluster 0: vulnerability allows user service access attacker versions prior discovered issue

	\item Cluster 1: needed android id privileges lead possible execution interaction exploitation bounds

	\item Cluster 2: macos issue addressed improved fixed ios ipados able 15 13

	\item Cluster 3: code vulnerability attacker remote execution arbitrary execute file exploit user

	\item \textcolor{red}{Cluster 4: site cross scripting xss plugin stored vulnerability wordpress
		      forgery csrf}

	\item Cluster 5:sql injection php parameter v1 vulnerability contain discovered
	      admin vulnerable

	\item Cluster 6: manipulation identifier leads vdb vulnerability classified unknown remotely attack disclosed

	\item Cluster 7: cvss oracle mysql vulnerability attacks server access base unauthorized score
\end{itemize}

The most promising from this initial set is cluster four, highlighted in red above. Cross-site
scripting (XSS) and wordpress plugins are a common trend within CVEs.
Figure~\ref{fig:cross_site_per_year} shows the counts per year of CVEs published containing at least five of the
words from that cluster. Five is arbitrary, this is more here just to show a potential insight in
looking at these trends. In this case, from ~2019 to 2023 there is a large increase in these types
of vulnerabilities, mainly in WordPress plugins. You may notice the trend matches with the general
trend of CVEs published (Figure~\ref{fig:cves_per_year}), this is a positive result, in that, the clustering
is still following the underlying distribution and has not failed to capture the overall trend.

Here are some example of the descriptions, for you viewing pleasure:

\begin{table}[h]
	\centering
	\begin{tabular}{|p{0.2\textwidth}|p{0.7\textwidth}|}
		\hline
		\textbf{CVE ID} & \textbf{Description}                                                    \\
		\hline

		CVE-2023-24378  & Auth. (contributor+) Stored Cross-Site Scripting (XSS) vulnerability in
		Codeat Glossary plugin $\leq$~2.1.27 versions.                                            \\

		\hline

		CVE-2023-24396  & Auth. (admin+) Stored Cross-Site Scripting (XSS) vulnerability in E4J
		s.R.L. VikBooking Hotel Booking Engine \& PMS plugin $\leq$~1.5.11 versions.              \\

		\hline

		CVE-2023-25062  & Auth. (admin+) Stored Cross-Site Scripting (XSS) vulnerability in
		PINPOINT.WORLD Pinpoint Booking System plugin $\leq$~2.9.9.2.8 versions.                  \\

		\hline
	\end{tabular}
	\caption{CVE Descriptions for Various WordPress Plugins}
	\label{tab:cve-descriptions}
\end{table}

This was a good indicator that it is worth looking into clustering further.

\begin{figure}[H]
	\centering

	\includegraphics[width=0.8\textwidth]{figures/cross_site_per_year.pdf}
	\caption{\label{fig:cross_site_per_year}CVEs with descriptions containing at least five of the
		words from cluster four}
\end{figure}

\subsection{Latent Dirichlet Allocation Methodology}

The next approach to topic modeling utilizes Latent Dirichlet Allocation (LDA)~\cite{lda_origin} in
conjunction with Word2Vec~\cite{word2vec} embeddings to analyze Common Vulnerabilities and Exposures
(CVE) descriptions. As an unsupervised learning method, LDA is well-suited for exploring large
collections of text data without predefined categories~\cite{lda_origin, latent_handbook}. This is
particularly valuable in the CVE context, where we aim to discover latent themes in vulnerability
descriptions without prior knowledge of these themes. LDA's soft clustering approach allows CVE
descriptions to belong to multiple topics with varying probabilities~\cite{latent_handbook}, which is beneficial given
the multi-faceted nature of many vulnerabilities. Furthermore, LDA produces human-interpretable
topics~\cite{lda_origin}, facilitating easier derivation of insights from the clustering results.
The paper \say{Security trend analysis with CVE topic models, by Neuhaus \&
	Zimmerman(2010)}\cite{cve_topic_modelling} uses this technique to great effect on the NVD
dataset (the analysis was for data ending in 2009) giving precedence to LDA being effective for this
sort of task.

The methodology comprises several key steps:

\subsubsection*{Data Preprocessing} I begin by preprocessing the CVE descriptions:

\begin{itemize}

	\item Custom stopwords are defined, combining standard English stopwords with domain-specific
	      terms (e.g., \say{vulnerability}, \say{attacker}).

	\item Each description is tokenized and filtered, removing stopwords and short tokens (length
	      $\leq 2$).

	\item A dictionary is created from the processed texts, filtering out extremely rare and
	      extremely common terms.

\end{itemize}

This data processing is necessary due to this type of topic discovery relying on word distributions,
and therefore word frequencies. As a result stop words just muddy the water and put more interesting
and relevant topic words further down.

\subsubsection*{Word Embedding}

A Word2Vec~\cite{word2vec} model is trained on the preprocessed texts:

\begin{itemize} \item Vector size: 100 \item Window size: 5 \item Minimum word count: 2
\end{itemize}

This embedding captures semantic relationships between words in the CVE context.

The parameters chosen here just felt like reasonable defaults and have not been explored. There are
also other options in using something like pretrained fasttext~\cite{fasttext} as used in
\cite{nvd_clustering_fasttext}.

% \subsubsection*{Topic Coherence Metric}

% \todo[inline]{As I haven't really used these in a rigorous way, I am unsure if I should keep this
% 	in, I have not gone into detail on these as I feel like it may be wasted effort if I do not use them
% 	too much }

% I define a custom coherence metric based on Word2Vec similarities:

% \begin{equation}
% 	Coherence = \frac{\sum_{i=1}^{n}\sum_{j=i+1}^{n} similarity(word_i, word_j)}{\binom{n}{2}}
% \end{equation}

% where $n$ is the number of top words in a topic (10 in my implementation).

% In addition I have kept track of the C-V coherence score used as default by Gensim, which comes from
% \cite{cv_coherence}

% \todo[inline]{Also have used perplexity, as a score as that was the common way in the original lda
% 	paper.... I haven't really used it though, just interesting in how unstable it was compared with the
% 	other coherence style metrics...}

\subsubsection*{LDA Model Training}


I have done many different iterations of the models with varying numbers of topics and
hyperparameters. In grid search attempts I found that the different coherences metrics did not agree
with each other. In addition, as we are searching for unknown topics in a unsupervised fashion, it is
dubious how helpful these measures of fit are in any case. My results of such exploration will be
found in the appendices. The hyperparameters below are broadly matching what I found to be the best,
however the main metric came from looking at the actual clustering results, and the biggest impact
came from the number of topics, with the final models being picked by their purity score (as
described in Section~\ref{sec:purity})

\begin{itemize}

	\item Number of topics: \{20 40 60 80 100\}

	\item $\alpha$: \{\say{symmetric}\}

	\item $\eta$: \{0.1\}

	\item Number of passes: \{30\}

	\item Number of iterations: \{200\}

\end{itemize}

The LDA model is implemented using Gensim's LdaMulticore\cite{gensim}, allowing for parallel
processing. In general I found Gensim very nice to work with, the model training was fast and the
API made sense.

% \subsubsection{Model Evaluation and Selection}

% Models are evaluated based on the average topic coherence:

% \begin{equation}
% 	Score = \frac{1}{T}\sum_{t=1}^{T} Coherence(topic_t)
% \end{equation}

% where $T$ is the total number of topics.


\subsubsection*{Topic Assignment and Analysis}

For each saved model:

\begin{itemize}

	\item Topic assignments are generated for each document in the corpus.

	\item Results, including document index, description, assigned topic, and CVSS data, are saved
	      in JSON format.

	\item The top 50 words for each topic are extracted and saved for interpretation.

\end{itemize}


% \subsubsection*{ Dataset Analysis}

% We run the LDA model on a balanced dataset to mitigate potential biases:

% \begin{itemize}

% 	\item The dataset is balanced by undersampling to ensure equal representation of each class
% 	      within a chosen CVSS metric (e.g., Confidentiality Impact) where possible.

% 	\item The LDA model is then applied to this balanced dataset using the previously determined parameters.

% \end{itemize}

\subsubsection*{Cluster-Class Association}

For each class within each CVSS metric, we identify the most representative cluster:

\begin{itemize}

	\item Calculate the proportion of documents from each class assigned to each topic cluster.

	\item The cluster with the highest count for a given class is considered its best representative.

	\item Calculate the purity score for all the different numbers of topics

\end{itemize}

With the best cluster from a given set discovered, we now need to decided which number of clusters
provides the best overall fit. A rudimentary approach is to just use the purity score.

\subsection{Purity as a Cluster Evaluation Metric}\label{sec:purity}

Purity is a simple external evaluation measure for cluster quality, particularly useful when
predefined classes are available for the data~\cite{purity_usuage}.

\subsubsection*{Definition and Calculation}

For a set of clusters $\Omega = \{\omega_1, \omega_2, ..., \omega_K\}$ and a set of classes $C =
	\{c_1, c_2, ..., c_J\}$, purity is defined as:

\begin{equation}
	\text{purity}(\Omega, C) = \frac{1}{N} \sum_{k=1}^K \max_j |\omega_k \cap c_j|
\end{equation}

where $N$ is the total number of objects, and $|\omega_k \cap c_j|$ is the number of objects in
cluster $\omega_k$ that belong to class $c_j$.

\subsubsection*{Interpretation}

Purity ranges from 0 to 1, where 1 indicates perfect purity. A higher purity value generally
suggests better clustering quality with respect to the ground truth classes\cite{purity_info_ret}.

In the context of topic modeling for CVSS metrics, high purity would indicate that each discovered
topic strongly corresponds to a specific CVSS category.

\subsubsection*{Limitations}

It's important to note that purity tends to increase as the number of clusters increases, with a
trivial solution of perfect purity when each object is in its own cluster \cite{v-measure}.
Therefore, it should be interpreted cautiously, especially when comparing models with different
numbers of topics, however, as seen by Figures~\ref{fig:purity_integrity},
\ref{fig:purity_20_availability}, \ref{fig:purity_20_confidentiality} this effect did not present
itself too strongly. This effect incentivise picking the number of topics around the elbow of the
graph (the point on a curve where the rate of change significantly shifts), as any minor increase in
the purity score could just be based on the inherent increase in the purity score as the number of
topics increase.

\begin{figure}[ht] \centering
	\includegraphics[width=1\textwidth]{figures/purity/topic_model_performance_purity_ground_truth_integrityImpact.pdf}
	\caption{\label{fig:purity_integrity}The purity score
		for when the topics have been assigned with a focus on \texttt{Integrity Impact}, split by the different
		possible classes}
\end{figure}

\begin{figure}[ht] \centering
	\includegraphics[width=1\textwidth]{figures/purity/topic_model_performance_purity_ground_truth_availabilityImpact.pdf}
	\caption{\label{fig:purity_20_availability}The purity score
		for when the topics have been assigned with a focus on \texttt{Availability Impact}, split by the different
		possible classes}
\end{figure}

\begin{figure}[ht] \centering
	\includegraphics[width=1\textwidth]{figures/purity/topic_model_performance_purity_ground_truth_confidentialityImpact.pdf}
	\caption{\label{fig:purity_20_confidentiality}The purity score
		for when the topics have been assigned with a focus on \texttt{Confidentiality Impact}, split by the different
		possible classes}
\end{figure}


\subsection{Evaluation and Insights}

\subsubsection*{Elephant in the Room, Data Imbalance}

As is obvious from Figure~\ref{fig:nvd_data}, the classes for each metric are highly imbalanced.
This is just a reality of the data and so is something to contend with. This aspect
made finding a well defined cluster for each topic difficult as well as making the graphing and
interpreting the outcome of the clustering somewhat awkward.

As an example, if we take look at Figure~\ref{fig:priviledgesRequired_BAD}, which shows the
clustering for Privileges Required  of the best cluster with a focus on the \texttt{HIGH} class, we
can see that even though we are trying to find the cluster with the best representation of
\texttt{HIGH} both of the other classes are more dominant. This is a result of the massive class
imbalance, the clustering cannot manage to find a class to represent such a little proportion of the
data. As a result the analysis will be focused on the metrics which are the most naturally balanced
as well as related to each other, these are the CIA triad, confidentiality, integrity, and availiability
impact.

\begin{figure}[ht]
	\centering
	\includegraphics[width=0.8\textwidth]{figures/privilegesRequired/merged_top_k_topics_category_focus_counts_privilegesRequired_HIGH_k1.pdf}

	\caption{The counts of the documents within the best topic in relation to Privileges Required with
		target class \texttt{HIGH}}

	\label{fig:priviledgesRequired_BAD}
\end{figure}

\subsection{Clustering Results}

The clustering analysis using Latent Dirichlet Allocation (LDA) yielded insights into the patterns
of vulnerabilities as they relate to the Common Vulnerability Scoring System (CVSS) metrics,
particularly the integrity impact metric. While an ideal analysis would involve comparing our
clusters with the assigned Common Weakness Enumeration (CWE), the time constraints and the complex
nature of both our topic-based clusters and the CWE descriptions made automated cross-referencing
unfeasible for the time being.

\subsubsection*{Methodology and Presentation}

The primary results of the analysis are presented in graphical form. Each graph shows the
distribution of documents across the three classes of integrity impact (\texttt{NONE}, \texttt{LOW},
\texttt{HIGH}) for the topic cluster that best represents each target class. This approach allows us
to visualize how well this clustering method separates vulnerabilities based on their integrity impact.

\todo[inline]{Fix the position of the myriad graphs that are in weird places in the document}
\subsubsection{Integrity Impact Analysis}

The \texttt{Integrity Impact} metric in CVSS refers to the degree to which a vulnerability, if exploited,
could affect the trustworthiness and veracity of data. A high integrity impact implies that data
could be significantly corrupted or altered, potentially leading to serious consequences for the
affected system or its users.

\subsubsection*{NONE Category}

Figure \ref{fig:integrityImpact_60_NONE} shows the distribution for the cluster best representing
the \texttt{NONE} category of integrity impact.

\begin{figure}[ht]
	\centering
	\includegraphics[width=0.8\textwidth]{figures/integrityImpact/merged_top_k_topics_category_focus_counts_integrityImpact_NONE_k1.pdf}

	\caption{The counts of the documents within the best topic in relation to \texttt{Integrity
			Impact} with
		target class \texttt{NONE}}

	\label{fig:integrityImpact_60_NONE}
\end{figure}

In this cluster, we observe a predominance of vulnerabilities related to denial of service attacks
and system crashes. While these issues are serious and can affect system availability, they
typically do not directly compromise data integrity. This aligns with the expectations for
vulnerabilities classified as having no integrity impact.

Key findings:
\begin{itemize}

	\item The cluster shows a clear majority of \texttt{NONE}-rated vulnerabilities.

	\item Common terms in this cluster likely include "denial of service", "crash", and
	      "dos".

	\item This result supports the effectiveness of our clustering in identifying vulnerabilities
	      with no integrity impact.

\end{itemize}

\subsubsection*{LOW Category}

Figure \ref{fig:integrityImpact_60_LOW} represents the distribution for the cluster best representing the \texttt{LOW} category of integrity impact.

\begin{figure}[ht]
	\centering
	\includegraphics[width=0.8\textwidth]{figures/integrityImpact/merged_top_k_topics_category_focus_counts_integrityImpact_LOW_k1.pdf}
	\caption{The counts of the documents within the best topic in relation to \texttt{Integrity Impact} with target class \texttt{LOW}}
	\label{fig:integrityImpact_60_LOW}
\end{figure}

This cluster predominantly features cross-site scripting (XSS) vulnerabilities and other generally
web browser based attacks. These types of vulnerabilities can indeed cause data integrity issues, but
they are usually limited in scope, typically affecting individual user interactions rather than
compromising the entire application's data integrity.

Key findings:
\begin{itemize}

	\item The cluster shows a majority of \texttt{LOW}-rated vulnerabilities, with some overlap into
	      other categories.

	\item Common terms likely include "cross-site scripting", "XSS", "javascript", and "html".

	\item This cluster aligns with our earlier k-means clustering results, providing some
	      similarities between the clustering
\end{itemize}

\subsubsection*{HIGH Category}

\todo[inline]{Needs a bit of work, when switching to purity this changed a little what was the best
	topic...}
Figure \ref{fig:integrityImpact_60_HIGH} shows the distribution for the cluster best representing the \texttt{HIGH} category of integrity impact.

\begin{figure}[ht]
	\centering
	\includegraphics[width=0.8\textwidth]{figures/integrityImpact/merged_top_k_topics_category_focus_counts_integrityImpact_HIGH_k1.pdf}
	\caption{The counts of the documents within the best topic in relation to integrityImpact with target class \texttt{HIGH}}
	\label{fig:integrityImpact_60_HIGH}
\end{figure}

In this cluster, we see a focus on remote code execution vulnerabilities. These
types of attacks have the potential to severely compromise data integrity by allowing unauthorized
modification or corruption of server contents.

Key findings:
\begin{itemize}

	\item The cluster shows a strong representation of \texttt{HIGH}-rated vulnerabilities.

	\item Common terms likely include "remote", "execution"

	\item The prevalence of these severe vulnerabilities in this cluster aligns with cybersecurity
	      expert expectations for high-impact integrity issues.

\end{itemize}

\begin{figure}[ht]
	\centering
	\includegraphics[width=0.8\textwidth]{figures/integrityImpact/integrityImpact_NONE_60_BAD.pdf}
	\caption{The counts of the documents within the best topic in relation to integrityImpact with
		target class \texttt{NONE}, for the number of clusters (60) with the lowest purity}
	\label{fig:integrityImpact_60_NONE_BAD}
\end{figure}

\todo[inline]{I was also planning on adding confidentiality and availability impact after here which
	I have not done yet, still a good idea? So that I can talk about them together a bit more}

\subsection{Cluster Merging and Refinement}

In an attempt to enhance class representation and potentially improve the overall performance of the
topic model, I explored the concept of merging related clusters. This process involved several steps:

\begin{itemize}

	\item Identifying clusters with semantic similarity or overlapping representation of CVSS classes.

	\item Merging these clusters by combining their topic-word distributions and reassigning documents.

	\item Recalculating class representation metrics for the merged clusters.

	\item Comparing the performance of the merged model to the original in terms of:

	      \begin{itemize}

		      \item Overall topic coherence

		      \item Clarity of class representation

		      \item Interpretability of resulting topics

	      \end{itemize}

\end{itemize}

\subsubsection{Methodology}

The approach to cluster merging was based on a simple similarity metric between topic-word
distributions. Clusters were merged if their similarity exceeded a predefined threshold. While this
method is straightforward to implement, it is admittedly crude and may not capture all the nuances
of semantic similarity between topics.

Other approaches exist, for instance, Neuhaus \& Zimmerman
(2010)~\cite{cve_topic_modelling} merged clusters based on the similarity of the topic words found
by their model. This other approach potentially offers a more nuanced way of identifying truly related topics.

\subsubsection{Results}

To illustrate the effects of cluster merging, we present the following figures:
\todo[inline]{need to add these in!}

% Insert figures here
% \begin{figure}[ht]
% \centering
% \includegraphics[width=0.8\textwidth]{path_to_original_cluster_figure}
% \caption{Distribution of CVSS classes in original clusters}
% \label{fig:original_clusters}
% \end{figure}

% \begin{figure}[ht]
% \centering
% \includegraphics[width=0.8\textwidth]{path_to_merged_cluster_figure}
% \caption{Distribution of CVSS classes in merged clusters}
% \label{fig:merged_clusters}
% \end{figure}

Contrary to our initial expectations, the process of merging clusters did not lead to improved
results. Instead, I observed the following effects:

The merging of clusters resulted in several negative impacts on the model's quality and utility. The
process led to decreased cluster purity, with a higher mix of different CVSS classes within each
cluster. This reduction in homogeneity made the larger, merged clusters more difficult to interpret,
diluting their defining characteristics and semantic meanings. Consequently, the model lost
granularity, as specific types of vulnerabilities or attack vectors were grouped into broader, less
nuanced categories. Overall, these changes contributed to a decrease in topic coherence, indicating
that the merged clusters were less internally consistent than their more granular predecessors.


\subsection{Interpretation and Future Directions}

These results suggest that our initial clustering approach was already capturing meaningful
distinctions between different types of vulnerabilities and their associated CVSS ratings. The
process of merging clusters, at least with the current methodology, appears to obscure these
distinctions rather than enhance them.

However, this does not necessarily mean that cluster merging is inherently unhelpful. Instead, it
suggests that more sophisticated merging strategies may be needed to preserve the valuable
information captured in the original clusters while still allowing for the combination of truly
related topics.

Possible future work on this topic includes:

\begin{itemize}

	\item Implementing the cluster merging approach of Neuhaus \& Zimmerman (2010), which focuses on
	      the similarity of topic words rather than overall distribution similarity.

	\item Exploring hierarchical topic modeling approaches, which might allow for the representation
	      of both fine-grained and broader topic categories simultaneously.

	\item Investigating dynamic topic modeling techniques to capture how vulnerability types and
	      their CVSS ratings evolve over time, potentially revealing patterns in how initially
	      distinct vulnerability categories merge or diverge over time.

\end{itemize}


\subsection{Overall Observations}

\begin{enumerate}

	\item Our clustering approach successfully differentiated between vulnerabilities with varying
	      levels of integrity impact, as evidenced by the distinct profiles of each cluster.

	\item The results align well with expert knowledge in the field of cybersecurity, suggesting
	      that our unsupervised learning approach can capture meaningful patterns in vulnerability
	      data.

	\item While our current analysis provides a static view of the vulnerability landscape, the
	      temporal aspect of the data presents an opportunity for future research. Analyzing how these
	      clusters and trends have evolved over time could provide additional insights into the
	      changing nature of cybersecurity threats.

	\item The overlap between categories in some clusters (particularly visible in the \texttt{LOW}
	      category) highlights the complexity of vulnerability classification and the potential for
	      ambiguity in CVSS scoring.

\end{enumerate}


% \section{Discussion}\label{discussion}


% \subsubsection{Exploit Prediction Scoring System}

% EPSS is developed by FIRST, the same group who govern the CVSS standard. It has a different take
% on the problem, focusing on a data driven model designed to give \say{a daily estimate of the probability
% 	of exploitation activity being observed over the next 30 days~\cite{EPSS}.} If the data shown on the model
% page is to be believed, it is a promising system (some of their findings Figure~\ref{fig:epss_and_cvss},
% Figure~\ref{fig:epss}). Unfortunately, while good to keep in mind for the industry, it is less useful
% for our purposes. This is a pretrained and uninterpretable model, from the outside at least.
% Analysis could be done on the output of the model in relation to CVEs, but that will not be a focus
% going forward.


\subsection{Future Work}

As I am a Masters student, I will be continuing the work that I have done here for COSC480 into next
year. My future work will focus on several key areas:

\begin{itemize}

	\item \textbf{Dataset Refinement:} The CVE dataset contains numerous poorly written descriptions
	      that hinder machine learning model performance. We plan to develop robust methods for
	      identifying and filtering out low-quality entries, potentially using the insights gained
	      from our clustering analysis.

	\item \textbf{Advanced Clustering Techniques:} Building on our current work, we aim to explore
	      more sophisticated clustering approaches. This includes implementing the cluster merging
	      method proposed by Neuhaus \& Zimmerman (2010)~\cite{cve_topic_modelling}, which focuses on
	      the similarity of topic words.

	\item \textbf{Temporal Analysis:} We intend to investigate how vulnerability patterns and their
	      CVSS ratings evolve over time. This could involve applying dynamic topic modeling techniques
	      to capture temporal trends in the data.

\end{itemize}

Through these efforts, I aim to develop a more robust, data-driven approach to vulnerability
analysis and CVSS scoring that can adapt to the evolving landscape of cybersecurity threats.

\section{Conclusion}

The Common Vulnerability Scoring System (CVSS) plays a vital role in prioritizing and managing the
ever-increasing number of software vulnerabilities. This research has highlighted both the strengths
and limitations of current CVSS implementation and scoring practices.

Key findings from our study include:

\begin{itemize}

	\item \textbf{Database Variability:} We observed variability in CVSS scores between
	      different databases, such as NVD and MITRE. This inconsistency underscores the subjective
	      nature of vulnerability scoring and the challenges in establishing a reliable ground truth.

	\item \textbf{Clustering Insights:} The application of Latent Dirichlet Allocation (LDA) for
	      clustering CVE descriptions revealed meaningful patterns in vulnerability types and their
	      associated CVSS ratings. This demonstrates the potential of unsupervised learning techniques
	      in capturing latent structures within vulnerability data.

	\item \textbf{Cluster Merging Limitations:} Our experiments with cluster merging, while not
	      yielding immediate improvements, provided valuable insights into the robustness of our
	      initial clustering and the complexity of semantic relationships between vulnerability types.

	\item \textbf{Data Quality Importance:} The analysis highlighted the critical role of data
	      quality in both manual and automated CVSS scoring processes. Poorly written or inconsistent
	      CVE descriptions pose significant challenges for accurate vulnerability assessment.

\end{itemize}

While CVSS remains a crucial tool for vulnerability management, our research suggests that its
current implementation has limitations that need to be addressed. The integration of machine
learning models, particularly those focused on natural language processing and topic modeling,
offers promising avenues for automating and enhancing the accuracy of CVSS scoring.

By combining advanced machine learning techniques with domain expertise in cybersecurity, we can
work towards a more consistent, accurate, and interpretable system for assessing and prioritizing
software vulnerabilities. This improved system would not only enhance the efficiency of
vulnerability management but also contribute to a more secure digital ecosystem overall.

% \subsection{Future Work}

% Despite the disadvantages of CVSS, I will continue to focus on it in my studies. My focus will pivot
% towards the interpretability of large language models, particularly from the perspective of data.
% The CVE dataset is messy, with many poorly written CVE descriptions that are not useful for machine
% learning models. I plan to clean up this dataset by identifying and removing low-quality entries. To
% achieve this, I will perform clustering and general analysis of the CVE descriptions to better
% understand the dataset and develop heuristics for filtering it.

% \section{Conclusion}

% The Common Vulnerability Scoring System (CVSS) is integral in prioritizing and managing the
% ever-growing number of software vulnerabilities. While the current approach heavily relies on
% manually assigned scores from the National Vulnerability Database (NVD), it may be worth using the
% MITRE database as well. However, the findings are very much inconclusive, it is possible that adding
% the extra data during model training is worth it, but so far it has only been to the detriment of
% performance. One key issue is the variability in CVSS scores between different databases, such as NVD
% and MITRE, which suggests a lack of consistency in the scoring process. This inconsistency can
% confuse automated systems and reduce the overall reliability of the predicted scores.

% In conclusion, while CVSS remains a crucial tool for vulnerability management, its current
% implementation has limitations that need to be addressed. The integration of machine learning models
% offers a promising solution to automate and enhance the accuracy of CVSS scoring. However the focus
% should be on predicting the distinct categorical variable for each metric, as this is a universally
% interesting classification task and will be more able to apply to changes in the CVSS standard.
% Future research should focus on refining these models, and focusing on interpretability, whether
% through LLM interpretability methods, or through more traditional databased clustering.

\printbibliography[title={References}]



\appendix

\renewcommand{\thesection}{Appendix \Alph{section}}

\subsection{CVSS figures from other versions}

Below is a collection of confusion matrics which are results from the Bayesian analysis of CVSS
versions 2.0 \& 3.0 for both the NVD and MITRE databases. Version 2.0 for MITRE is especially rough
as there was very little data.

\begin{figure}[H]
	\centering
	\makebox[\textwidth][c]{\includegraphics[width=1.5\textwidth]{{figures/nvd_2_titles.pdf}}}%
	\caption{\label{fig:nvd_2_confusion_matrices}Confusion matrix of estimated accuracy for CVSS metrics for version 2.0 for NVD}
\end{figure}

\begin{figure}[H]
	\centering
	\makebox[\textwidth][c]{\includegraphics[width=1.5\textwidth]{figures/mitre_2_titles.pdf}}
	\caption{\label{fig:mitre_2_confusion_matrices}Confusion matrix of estimated accuracy for CVSS
		metrics for version 2.0 for MITRE}
\end{figure}

\begin{figure}[H]
	\centering
	\makebox[\textwidth][c]{\includegraphics[width=1.5\textwidth]{figures/nvd_30_titles.pdf}}
	\caption{\label{fig:nvd_30_confusion_matrices}Confusion matrix of estimated accuracy for CVSS metrics for version 3.0 for NVD}
\end{figure}

\begin{figure}[H]
	\centering
	\makebox[\textwidth][c]{\includegraphics[width=1.5\textwidth]{figures/mitre_30_titles.pdf}}
	\caption{\label{fig:mitre_30_confusion_matrices}Confusion matrix of estimated accuracy for CVSS
		metrics for version 3.0 for MITRE}
\end{figure}



\subsection{CVSS 3.1 Base Score formula} \label{equation}

Below is a formulaic representation of the CVSS 3.1 base score formula.
	{\scriptsize
		\begin{equation}
			ISS = 1 - ((1 - Confidentiality) \times (1 - Integrity) \times (1 - Availability)) \\
		\end{equation}
		\begin{equation}
			Impact =
			\begin{cases}
				7.52 \times (ISS - 0.029) - 3.25 \times (ISS - 0.02)^{15} & \text{if \textit{Scope} is
				\textit{Changed}}                                                                      \\
				6.42 \times ISS                                           & \text{if \textit{Scope} is
					\textit{Unchanged}}
			\end{cases}
		\end{equation}

		\begin{equation}
			Exploitability = 8.22 - AttackVector \times AttackComplexity \times PrivilegesRequired \times
			UserInteraction \\
		\end{equation}

		\begin{equation}
			Base Score =
			\begin{cases}
				0                                                           & Impact \leq 0              \\
				Roundup(Minimum(1.08 \times (Impact + Exploitability), 10)) & \text{if \textit{Scope} is
				\textit{Changed}}                                                                        \\
				Roundup(Minimum((Impact + Exploitability), 0))              & \text{if \textit{Scope} is
				\textit{Unchanged}}                                                                      \\
			\end{cases}
		\end{equation}
	}

\subsection{Exploit Prediction Scoring System diagrams for reference}

Below are two graphs from FIRST, the creator of EPSS. These show some of the results they have
found for this system, especially when comparing EPSS to CVSS in terms of effeciency of effort if
you use EPSS to guide remediation of vulnerabilities.

\begin{figure}
	\centering
	\makebox[\textwidth][c]{\includegraphics[width=1\textwidth]{figures/epss.pdf}}
	\caption{\label{fig:epss}Comparing Metrics: CVSS 7+ vs. EPSS 10\%+ sourced from \cite{EPSS}}
\end{figure}

\begin{figure}
	\centering
	\makebox[\textwidth][c]{\includegraphics[width=1\textwidth]{figures/epss_and_cvss.pdf}}
	\caption{\label{fig:epss_and_cvss}EPSS score compared to CVSS Base Score (NVD) sourced from
		\cite{EPSS_USER}}
\end{figure}

\section{Aims and Objectives}

\subsection*{Original}

\paragraph{Aims}
The primary aim of this research is to develop sophisticated predictive models capable of accurately determining
the severity levels of security threats based on the CVSS. This will involve a comprehensive review and comparison
of current datasets, with a focus on leveraging natural language descriptions provided in security vulnerability reports.
The project intends to utilize advanced transformer-based models to achieve this goal, contributing to the field of
cybersecurity by enhancing the precision of threat severity assessments.

\paragraph{Objectives}
\begin{itemize}[noitemsep]
	\item Conduct a comprehensive literature review to understand the current landscape of CVSS score prediction and the methodologies employed in existing models.
	\item Replicate successful methodologies to verify the accuracy of CVSS score databases, with a particular focus on alignment with recent CVSS standards and datasets.
	\item Explore opportunities for enhancing existing methodologies, including the investigation of data amalgamation from multiple databases to ascertain improvements in model performance.
	\item Experiment with various model architectures to identify the most effective approach in terms of predictive accuracy, specifically focusing on metrics such as the F1 score and balanced accuracy.
\end{itemize}

\paragraph{Timeline}
\begin{itemize}[noitemsep]
	\item March: Initiate the project with a literature review, system environment setup, and resource gathering.
	\item March-April: Replicate existing methodologies to validate findings and ensure alignment with current standards.
	\item May-June: Generate preliminary results and compile an interim report detailing findings and methodologies.
	\item July-August: Conduct experiments with various data source combinations and model architectures to identify optimal configurations.
	\item September-October: Finalize experimental work, analyze results, and prepare the comprehensive final report.
\end{itemize}

\subsection*{Revised}

\paragraph{Aims}

The primary aim of this research is to develop sophisticated predictive models capable of accurately determining
the severity levels of security threats based on the CVSS. This will involve a comprehensive review and comparison
of current datasets, with a focus on leveraging natural language descriptions provided in security vulnerability reports.
The project intends to utilize advanced transformer-based models to achieve this goal, contributing to the field of
cybersecurity by enhancing the precision of threat severity assessments.

\paragraph{Objectives}
\begin{itemize}[noitemsep]

	\item Conduct a comprehensive literature review to understand the current landscape of CVSS
	      score prediction and the methodologies employed in existing models.

	\item Replicate successful methodologies to verify the accuracy of CVSS score databases, with a
	      particular focus on alignment with recent CVSS standards and datasets.

	\item Explore opportunities for enhancing existing methodologies, including the investigation of
	      data amalgamation from multiple databases to ascertain improvements in model performance.

	\item Look into data cleaning and clustering, to improve the efficacy of the models, as well as
	      a look into interpretability though data analysis.

\end{itemize}

\end{document}

